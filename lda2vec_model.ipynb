{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/defeater/Documents/NLP/lda2vec/')\n",
    "sys.path.append('/home/defeater/Documents/NLP/lda2vec/lda2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lda2vec import preprocess, Corpus\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    import seaborn\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Error calculating span: Can't find end",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3413541ac8ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Also try running python -m spacy.en.download all --force\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/defeater/Documents/NLP/lda2vec/lda2vec/preprocess.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(texts, max_length, skip, attr, merge, nlp, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;31m# Only keep adjectives and nouns, e.g. \"good ideas\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep_\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbad_deps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                     \u001b[0mphrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphrase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/defeater/.local/lib/python3.5/site-packages/spacy/tokens/span.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span.__len__ (spacy/tokens/span.cpp:3955)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/defeater/.local/lib/python3.5/site-packages/spacy/tokens/span.pyx\u001b[0m in \u001b[0;36mspacy.tokens.span.Span._recalculate_indices (spacy/tokens/span.cpp:5105)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Error calculating span: Can't find end"
     ]
    }
   ],
   "source": [
    "from lda2vec import preprocess, Corpus\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pickle as pickle\n",
    "import os.path\n",
    "\n",
    "logging.basicConfig()\n",
    "\n",
    "max_length = 250   # Limit of 250 words per comment\n",
    "min_author_comments = 50  # Exclude authors with fewer comments\n",
    "nrows = None  # Number of rows of file to read; None reads in full file\n",
    "\n",
    "fn = \"hacker_news_comments.csv\"\n",
    "#url = \"https://zenodo.org/record/45901/files/hacker_news_comments.csv\"\n",
    "\n",
    "features = []\n",
    "# Convert to unicode (spaCy only works with unicode)\n",
    "features = pd.read_csv(fn, encoding='utf8', nrows=nrows)\n",
    "# Convert all integer arrays to int32\n",
    "for col, dtype in zip(features.columns, features.dtypes):\n",
    "    if dtype is np.dtype('int64'):\n",
    "        features[col] = features[col].astype('int32')\n",
    "\n",
    "# Tokenize the texts\n",
    "# If this fails it's likely spacy. Install a recent spacy version.\n",
    "# Only the most recent versions have tokenization of noun phrases\n",
    "# I'm using SHA dfd1a1d3a24b4ef5904975268c1bbb13ae1a32ff\n",
    "# Also try running python -m spacy.en.download all --force\n",
    "texts = features.pop('comment_text').values\n",
    "tokens, vocab = preprocess.tokenize(texts, max_length, n_threads=4, merge=True)\n",
    "del textsf\n",
    "\n",
    "# Make a ranked list of rare vs frequent words\n",
    "corpus = Corpus()\n",
    "corpus.update_word_count(tokens)\n",
    "corpus.finalize()\n",
    "\n",
    "# The tokenization uses spaCy indices, and so may have gaps\n",
    "# between indices for words that aren't present in our dataset.\n",
    "# This builds a new compact index\n",
    "compact = corpus.to_compact(tokens)\n",
    "# Remove extremely rare words\n",
    "pruned = corpus.filter_count(compact, min_count=10)\n",
    "# Words tend to have power law frequency, so selectively\n",
    "# downsample the most prevalent words\n",
    "clean = corpus.subsample_frequent(pruned)\n",
    "\n",
    "# Extract numpy arrays over the fields we want covered by topics\n",
    "# Convert to categorical variables\n",
    "author_counts = features['comment_author'].value_counts()\n",
    "to_remove = author_counts[author_counts < min_author_comments].index\n",
    "mask = features['comment_author'].isin(to_remove).values\n",
    "author_name = features['comment_author'].values.copy()\n",
    "author_name[mask] = 'infrequent_author'\n",
    "features['comment_author'] = author_name\n",
    "authors = pd.Categorical(features['comment_author'])\n",
    "author_id = authors.codes\n",
    "author_name = authors.categories\n",
    "story_id = pd.Categorical(features['story_id']).codes\n",
    "# Chop timestamps into days\n",
    "story_time = pd.to_datetime(features['story_time'], unit='s')\n",
    "days_since = (story_time - story_time.min()) / pd.Timedelta('1 day')\n",
    "time_id = days_since.astype('int32')\n",
    "features['story_id_codes'] = story_id\n",
    "features['author_id_codes'] = story_id\n",
    "features['time_id_codes'] = time_id\n",
    "\n",
    "# Extract outcome supervised features\n",
    "ranking = features['comment_ranking'].values\n",
    "score = features['story_comment_count'].values\n",
    "\n",
    "# Now flatten a 2D array of document per row and word position\n",
    "# per column to a 1D array of words. This will also remove skips\n",
    "# and OoV words\n",
    "feature_arrs = (story_id, author_id, time_id, ranking, score)\n",
    "flattened, features_flat = corpus.compact_to_flat(pruned, *feature_arrs)\n",
    "# Flattened feature arrays\n",
    "(story_id_f, author_id_f, time_id_f, ranking_f, score_f) = features_flat\n",
    "\n",
    "# Save the data\n",
    "pickle.dump(corpus, open('corpus', 'w'), protocol=2)\n",
    "pickle.dump(vocab, open('vocab', 'w'), protocol=2)\n",
    "features.to_pickle('features.pd')\n",
    "data = dict(flattened=flattened, story_id=story_id_f, author_id=author_id_f,\n",
    "            time_id=time_id_f, ranking=ranking_f, score=score_f,\n",
    "            author_name=author_name, author_index=author_id)\n",
    "np.savez('data', **data)\n",
    "np.save(open('tokens', 'w'), tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story_time</th>\n",
       "      <th>story_url</th>\n",
       "      <th>story_text</th>\n",
       "      <th>story_author</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>comment_author</th>\n",
       "      <th>comment_ranking</th>\n",
       "      <th>author_comment_count</th>\n",
       "      <th>story_comment_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3985069</th>\n",
       "      <td>1337221782</td>\n",
       "      <td>http://androidcommunity.com/verizon-killing-of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>joedev</td>\n",
       "      <td>3985756</td>\n",
       "      <td>Makes sense for Verizon, and if they are being...</td>\n",
       "      <td>Quizzy</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481190</th>\n",
       "      <td>1303738766</td>\n",
       "      <td>http://www.bbc.co.uk/news/magazine-13140772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>soitgoes</td>\n",
       "      <td>2481521</td>\n",
       "      <td>\"Made to play\" is a contradiction.</td>\n",
       "      <td>petervandijck</td>\n",
       "      <td>9</td>\n",
       "      <td>1125</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6302825</th>\n",
       "      <td>1377881848</td>\n",
       "      <td>https://gingkoapp.com/p/future-of-text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>adriano_f</td>\n",
       "      <td>6303075</td>\n",
       "      <td>I like this a lot!&lt;p&gt;The research manuscript e...</td>\n",
       "      <td>heurist</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268480</th>\n",
       "      <td>1377347411</td>\n",
       "      <td>http://lists.w3.org/Archives/Public/www-archiv...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bpierre</td>\n",
       "      <td>6270567</td>\n",
       "      <td>Fetching stuff over HTTP can be incredibly stu...</td>\n",
       "      <td>MBCook</td>\n",
       "      <td>5</td>\n",
       "      <td>84</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5487883</th>\n",
       "      <td>1365012711</td>\n",
       "      <td>https://medium.com/i-m-h-o/ef4772e3c628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jpadilla_</td>\n",
       "      <td>5487972</td>\n",
       "      <td>Conversely, start working( harder ).</td>\n",
       "      <td>TheSOB88</td>\n",
       "      <td>43</td>\n",
       "      <td>171</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          story_time                                          story_url  \\\n",
       "story_id                                                                  \n",
       "3985069   1337221782  http://androidcommunity.com/verizon-killing-of...   \n",
       "2481190   1303738766        http://www.bbc.co.uk/news/magazine-13140772   \n",
       "6302825   1377881848             https://gingkoapp.com/p/future-of-text   \n",
       "6268480   1377347411  http://lists.w3.org/Archives/Public/www-archiv...   \n",
       "5487883   1365012711            https://medium.com/i-m-h-o/ef4772e3c628   \n",
       "\n",
       "         story_text story_author  comment_id  \\\n",
       "story_id                                       \n",
       "3985069         NaN       joedev     3985756   \n",
       "2481190         NaN     soitgoes     2481521   \n",
       "6302825         NaN    adriano_f     6303075   \n",
       "6268480         NaN      bpierre     6270567   \n",
       "5487883         NaN    jpadilla_     5487972   \n",
       "\n",
       "                                               comment_text comment_author  \\\n",
       "story_id                                                                     \n",
       "3985069   Makes sense for Verizon, and if they are being...         Quizzy   \n",
       "2481190                  \"Made to play\" is a contradiction.  petervandijck   \n",
       "6302825   I like this a lot!<p>The research manuscript e...        heurist   \n",
       "6268480   Fetching stuff over HTTP can be incredibly stu...         MBCook   \n",
       "5487883                Conversely, start working( harder ).       TheSOB88   \n",
       "\n",
       "          comment_ranking  author_comment_count  story_comment_count  \n",
       "story_id                                                              \n",
       "3985069                10                    11                   13  \n",
       "2481190                 9                  1125                   16  \n",
       "6302825                 5                    31                   42  \n",
       "6268480                 5                    84                   11  \n",
       "5487883                43                   171                   44  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "fn = \"hacker_news_comments.csv\"\n",
    "df = DataFrame.from_csv(fn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from lda2vec import EmbedMixture\n",
    "from lda2vec import dirichlet_likelihood\n",
    "from lda2vec.utils import move\n",
    "\n",
    "from chainer import Chain\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LDA2Vec(Chain):\n",
    "    def __init__(self, n_stories=100, n_story_topics=10,\n",
    "                 n_authors=100, n_author_topics=10,\n",
    "                 n_units=256, n_vocab=1000, dropout_ratio=0.5, train=True,\n",
    "                 counts=None, n_samples=15, word_dropout_ratio=0.0):\n",
    "        em1 = EmbedMixture(n_stories, n_story_topics, n_units,\n",
    "                           dropout_ratio=dropout_ratio)\n",
    "        em2 = EmbedMixture(n_authors, n_author_topics, n_units,\n",
    "                           dropout_ratio=dropout_ratio)\n",
    "        kwargs = {}\n",
    "        kwargs['mixture_sty'] = em1\n",
    "        kwargs['mixture_aut'] = em2\n",
    "        kwargs['sampler'] = L.NegativeSampling(n_units, counts, n_samples)\n",
    "        super(LDA2Vec, self).__init__(**kwargs)\n",
    "        rand = np.random.random(self.sampler.W.data.shape)\n",
    "        self.sampler.W.data[:, :] = rand[:, :]\n",
    "        self.n_units = n_units\n",
    "        self.train = train\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.word_dropout_ratio = word_dropout_ratio\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def prior(self):\n",
    "        dl1 = dirichlet_likelihood(self.mixture_sty.weights)\n",
    "        dl2 = dirichlet_likelihood(self.mixture_aut.weights)\n",
    "        return dl1 + dl2\n",
    "\n",
    "    def fit_partial(self, rsty_ids, raut_ids, rwrd_ids, window=5):\n",
    "        sty_ids, aut_ids, wrd_ids = move(self.xp, rsty_ids, raut_ids, rwrd_ids)\n",
    "        pivot_idx = next(move(self.xp, rwrd_ids[window: -window]))\n",
    "        pivot = F.embed_id(pivot_idx, self.sampler.W)\n",
    "        sty_at_pivot = rsty_ids[window: -window]\n",
    "        aut_at_pivot = raut_ids[window: -window]\n",
    "        sty = self.mixture_sty(next(move(self.xp, sty_at_pivot)))\n",
    "        aut = self.mixture_aut(next(move(self.xp, aut_at_pivot)))\n",
    "        loss = 0.0\n",
    "        start, end = window, rwrd_ids.shape[0] - window\n",
    "        context = sty + aut + F.dropout(pivot, self.dropout_ratio)\n",
    "        for frame in range(-window, window + 1):\n",
    "            # Skip predicting the current pivot\n",
    "            if frame == 0:\n",
    "                continue\n",
    "            # Predict word given context and pivot word\n",
    "            # The target starts before the pivot\n",
    "            targetidx = rwrd_ids[start + frame: end + frame]\n",
    "            sty_at_target = rsty_ids[start + frame: end + frame]\n",
    "            aut_at_target = raut_ids[start + frame: end + frame]\n",
    "            sty_is_same = sty_at_target == sty_at_pivot\n",
    "            aut_is_same = aut_at_target == aut_at_pivot\n",
    "            # Randomly dropout words (default is to never do this)\n",
    "            rand = np.random.uniform(0, 1, sty_is_same.shape[0])\n",
    "            mask = (rand > self.word_dropout_ratio).astype('bool')\n",
    "            sty_and_aut_are_same = np.logical_and(sty_is_same, aut_is_same)\n",
    "            weight = np.logical_and(sty_and_aut_are_same, mask).astype('int32')\n",
    "            # If weight is 1.0 then targetidx\n",
    "            # If weight is 0.0 then -1\n",
    "            targetidx = targetidx * weight + -1 * (1 - weight)\n",
    "            target, = move(self.xp, targetidx)\n",
    "            loss = self.sampler(context, target)\n",
    "            loss.backward()\n",
    "        return loss.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Author: Chris Moody <chrisemoody@gmail.com>\n",
    "# License: MIT\n",
    "\n",
    "# This simple example loads the newsgroups data from sklearn\n",
    "# and train an LDA-like model on it\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "from chainer import serializers\n",
    "import chainer.optimizers as O\n",
    "import numpy as np\n",
    "\n",
    "from lda2vec import utils\n",
    "from lda2vec import prepare_topics, print_top_words_per_topic\n",
    "from lda2vec_model import LDA2Vec\n",
    "\n",
    "gpu_id = int(os.getenv('CUDA_GPU', 0))\n",
    "cuda.get_device(gpu_id).use()\n",
    "print \"Using GPU \" + str(gpu_id)\n",
    "\n",
    "# You must run preprocess.py before this data becomes available\n",
    "vocab = pickle.load(open('../data/vocab', 'r'))\n",
    "corpus = pickle.load(open('../data/corpus', 'r'))\n",
    "data = np.load(open('../data/data.npz', 'r'))\n",
    "flattened = data['flattened']\n",
    "story_id = data['story_id']\n",
    "author_id = data['author_id']\n",
    "time_id = data['time_id']\n",
    "ranking = data['ranking'].astype('float32')\n",
    "score = data['score'].astype('float32')\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "# Number of documents\n",
    "n_stories = story_id.max() + 1\n",
    "# Number of users\n",
    "n_authors = author_id.max() + 1\n",
    "# Number of unique words in the vocabulary\n",
    "n_vocab = flattened.max() + 1\n",
    "# Number of dimensions in a single word vector\n",
    "n_units = 256\n",
    "# Number of topics to fit\n",
    "n_story_topics = 40\n",
    "n_author_topics = 20\n",
    "batchsize = 4096\n",
    "# Get the string representation for every compact key\n",
    "words = corpus.word_list(vocab)[:n_vocab]\n",
    "\n",
    "# How many tokens are in each story\n",
    "sty_idx, lengths = np.unique(story_id, return_counts=True)\n",
    "sty_len = np.zeros(sty_idx.max() + 1, dtype='int32')\n",
    "sty_len[sty_idx] = lengths\n",
    "\n",
    "# How many tokens are in each author\n",
    "aut_idx, lengths = np.unique(author_id, return_counts=True)\n",
    "aut_len = np.zeros(aut_idx.max() + 1, dtype='int32')\n",
    "aut_len[aut_idx] = lengths\n",
    "\n",
    "# Count all token frequencies\n",
    "tok_idx, freq = np.unique(flattened, return_counts=True)\n",
    "term_frequency = np.zeros(n_vocab, dtype='int32')\n",
    "term_frequency[tok_idx] = freq\n",
    "\n",
    "model = LDA2Vec(n_stories=n_stories, n_story_topics=n_story_topics,\n",
    "                n_authors=n_authors, n_author_topics=n_author_topics,\n",
    "                n_units=n_units, n_vocab=n_vocab, counts=term_frequency,\n",
    "                n_samples=15)\n",
    "if os.path.exists('lda2vec.hdf5'):\n",
    "    print \"Reloading from saved\"\n",
    "    serializers.load_hdf5(\"lda2vec.hdf5\", model)\n",
    "model.to_gpu()\n",
    "optimizer = O.Adam()\n",
    "optimizer.setup(model)\n",
    "clip = chainer.optimizer.GradientClipping(5.0)\n",
    "optimizer.add_hook(clip)\n",
    "\n",
    "j = 0\n",
    "epoch = 0\n",
    "fraction = batchsize * 1.0 / flattened.shape[0]\n",
    "for epoch in range(5000):\n",
    "    ts = prepare_topics(cuda.to_cpu(model.mixture_sty.weights.W.data).copy(),\n",
    "                        cuda.to_cpu(model.mixture_sty.factors.W.data).copy(),\n",
    "                        cuda.to_cpu(model.sampler.W.data).copy(),\n",
    "                        words)\n",
    "    print_top_words_per_topic(ts)\n",
    "    ts['doc_lengths'] = sty_len\n",
    "    ts['term_frequency'] = term_frequency\n",
    "    np.savez('topics.story.pyldavis', **ts)\n",
    "    ta = prepare_topics(cuda.to_cpu(model.mixture_aut.weights.W.data).copy(),\n",
    "                        cuda.to_cpu(model.mixture_aut.factors.W.data).copy(),\n",
    "                        cuda.to_cpu(model.sampler.W.data).copy(),\n",
    "                        words)\n",
    "    print_top_words_per_topic(ta)\n",
    "    ta['doc_lengths'] = aut_len\n",
    "    ta['term_frequency'] = term_frequency\n",
    "    np.savez('topics.author.pyldavis', **ta)\n",
    "    for s, a, f in utils.chunks(batchsize, story_id, author_id, flattened):\n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grads()\n",
    "        l = model.fit_partial(s.copy(), a.copy(), f.copy())\n",
    "        prior = model.prior()\n",
    "        loss = prior * fraction\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        msg = (\"J:{j:05d} E:{epoch:05d} L:{loss:1.3e} \"\n",
    "               \"P:{prior:1.3e} R:{rate:1.3e}\")\n",
    "        prior.to_cpu()\n",
    "        loss.to_cpu()\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        rate = batchsize / dt\n",
    "        logs = dict(loss=float(l), epoch=epoch, j=j,\n",
    "                    prior=float(prior.data), rate=rate)\n",
    "        print msg.format(**logs)\n",
    "        j += 1\n",
    "    serializers.save_hdf5(\"lda2vec.hdf5\", model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
