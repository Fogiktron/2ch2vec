{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from re import sub, compile\n",
    "from requests import get, exceptions\n",
    "from pandas import DataFrame, set_option\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pylab import rcParams, savefig\n",
    "import matplotlib\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models import doc2vec\n",
    "from nltk import sent_tokenize\n",
    "from itertools import chain\n",
    "from nltk.tokenize import wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import models, corpora\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pickle import load\n",
    "from collections import namedtuple\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Утилсы: регулярки, загрузка файлов и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cut(data):\n",
    "    r = compile(r'<.*?>|>>.*|&#(47|92);|&quot;|&gt;|(http|https):.*')\n",
    "    return r.sub('', data)\n",
    "\n",
    "def punctuate(data):\n",
    "    r = compile( r'([a-zA-Z])([,.!])', r'\\1 \\2')\n",
    "    return r.sub('', data)\n",
    "\n",
    "def pos_cut(s):\n",
    "    s = sub(r\"ADJF\", \"ADJ\", s)\n",
    "    s = sub(r\"ADVB\", \"ADV\", s)\n",
    "    s = sub(r\"INFN\", \"VERB\", s)\n",
    "    return s\n",
    "\n",
    "def topic_cut(word):\n",
    "    return sub('[^a-zA-Zа-яА-Я]', '', word)\n",
    "\n",
    "def remove_noise(s):\n",
    "    remove_noise = compile('(@[A-Za-z_0-9]+)|\\[.+]\\,|\\[.+]|([^0-9A-Za-zА-Яа-я[\\.’,?!-:] \\t])'\n",
    "                           '|(\\w+:\\/\\/\\S+)')\n",
    "    return (sub(remove_noise, '',  s))\n",
    "\n",
    "def check_for_rus_lan(word):\n",
    "    regex = compile('[А-Яа-я]+$')\n",
    "    if regex.match(word):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "documents = load(open('pickle/pr.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализация стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('russian')\n",
    "stop += stopwords.words('english')\n",
    "my_stop = ['весь', 'всякий', 'тот', 'это', 'самое', 'каким', 'сама', 'никак', 'она', 'она', 'оно', 'какие', 'какого', 'которая', 'многое', 'чему', 'всему', 'раза', 'сразу', 'весь', 'раз', 'пор', 'например', 'вроде', 'которые', 'который', 'просто', 'очень', 'почему', 'вообще', 'ещё', 'типа', 'ради', 'всё', 'хотя', 'тебе', 'нужно', 'пока']\n",
    "my_stop_verb = ['сделать', 'можешь', 'могу', 'могут', 'делать', 'будешь', 'быть', 'будут', 'смочь']\n",
    "[stop.append(morph.parse(i)[0].normal_form) for i in my_stop]\n",
    "[stop.append(morph.parse(i)[0].normal_form) for i in my_stop_verb]\n",
    "dvach_stop = ['такое', 'лол', 'бля']\n",
    "stop += dvach_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Загрузка токенизатора, токенизация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('russian')\n",
    "alpha_tokenizer = RegexpTokenizer('[A-Za-zА-Яа-я]\\w+')\n",
    "\n",
    "def my_tokenize(sent):\n",
    "    sents = list(alpha_tokenizer.tokenize(sent))\n",
    "    return set([morph.parse(sent.lower())[0].normal_form for sent in sents if sent.lower() not in stop and len(sent.lower()) > 1])\n",
    "\n",
    "model_data = [my_tokenize(sent) for sent in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Обучение Word2Vec-модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "size = 800\n",
    "window = 10\n",
    "\n",
    "model = models.Word2Vec(model_data, min_count=min_count, size=size, window=window,  workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сериализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('2ch_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
