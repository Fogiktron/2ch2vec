{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from re import sub, compile\n",
    "from requests import get, exceptions\n",
    "from pandas import DataFrame, set_option\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pylab import rcParams, savefig\n",
    "import matplotlib\n",
    "from gensim.utils import tokenize\n",
    "from gensim.models import doc2vec\n",
    "from nltk import sent_tokenize\n",
    "from itertools import chain\n",
    "from nltk.tokenize import wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import models, corpora\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pickle import load\n",
    "from collections import namedtuple\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Утилсы: регулярки, загрузка файлов и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cut(data):\n",
    "    r = compile(r'<.*?>|>>.*|&#(47|92);|&quot;|&gt;|(http|https):.*')\n",
    "    return r.sub('', data)\n",
    "\n",
    "def punctuate(data):\n",
    "    r = compile( r'([a-zA-Z])([,.!])', r'\\1 \\2')\n",
    "    return r.sub('', data)\n",
    "\n",
    "def pos_cut(s):\n",
    "    s = sub(r\"ADJF\", \"ADJ\", s)\n",
    "    s = sub(r\"ADVB\", \"ADV\", s)\n",
    "    s = sub(r\"INFN\", \"VERB\", s)\n",
    "    return s\n",
    "\n",
    "def topic_cut(word):\n",
    "    return sub('[^a-zA-Zа-яА-Я]', '', word)\n",
    "\n",
    "def remove_noise(s):\n",
    "    remove_noise = compile('(@[A-Za-z_0-9]+)|\\[.+]\\,|\\[.+]|([^0-9A-Za-zА-Яа-я[\\.’,?!-:] \\t])'\n",
    "                           '|(\\w+:\\/\\/\\S+)')\n",
    "    return (sub(remove_noise, '',  s))\n",
    "\n",
    "def check_for_rus_lan(word):\n",
    "    regex = compile('[А-Яа-я]+$')\n",
    "    if regex.match(word):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "documents = load(open('pickle/pr.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Загрузка токенизатора, токенизация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('russian')\n",
    "alpha_tokenizer = RegexpTokenizer('[A-Za-zА-Яа-я]\\w+')\n",
    "\n",
    "def my_tokenize(sent):\n",
    "    sents = list(alpha_tokenizer.tokenize(sent))\n",
    "    return set([morph.parse(sent.lower())[0].normal_form for sent in sents if sent.lower() not in stop and len(sent.lower()) > 1])\n",
    "\n",
    "model_data = [my_tokenize(sent) for sent in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Инициализация стоп-слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('russian')\n",
    "stop += stopwords.words('english')\n",
    "my_stop = ['весь', 'всякий', 'тот', 'это', 'самое', 'каким', 'сама', 'никак', 'она', 'она', 'оно', 'какие', 'какого', 'которая', 'многое', 'чему', 'всему', 'раза', 'сразу', 'весь', 'раз', 'пор', 'например', 'вроде', 'которые', 'который', 'просто', 'очень', 'почему', 'вообще', 'ещё', 'типа', 'ради', 'всё', 'хотя', 'тебе', 'нужно', 'пока']\n",
    "my_stop_verb = ['сделать', 'можешь', 'могу', 'могут', 'делать', 'будешь', 'быть', 'будут', 'смочь']\n",
    "[stop.append(morph.parse(i)[0].normal_form) for i in my_stop]\n",
    "[stop.append(morph.parse(i)[0].normal_form) for i in my_stop_verb]\n",
    "dvach_stop = ['такое', 'лол', 'бля']\n",
    "stop += dvach_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Word2Vec-модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "min_count = 5\n",
    "size = 800\n",
    "window = 20\n",
    "\n",
    "model = models.Word2Vec(model_data, min_count=min_count, size=size, window=window,  workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word = morph.parse('кодить')[0].normal_form\n",
    "print(word)\n",
    "keyword = morph.parse('программирование')[0].normal_form\n",
    "\n",
    "model.similarity(word, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for test_word in documents[10000:10050]:\n",
    "    amount = 0\n",
    "    similarity = 0\n",
    "    for i in wordpunct_tokenize(test_word):\n",
    "        sim = 0\n",
    "        try:\n",
    "            sim = model.similarity(morph.parse(i)[0].normal_form, keyword)\n",
    "            amount += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        similarity += sim\n",
    "    positive = False\n",
    "    sim_am = 0\n",
    "    try:\n",
    "        sim_am = similarity/amount\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    if sim_am > 0.5:\n",
    "        positive = True\n",
    "    result.append({'text': test_word, 'similarity': sim_am, 'positive': positive})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "DataFrame.from_records(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Получение TF-IDF весов для корпуса и инициализация словаря, в котором каждому слову ассоциирован вес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corpus = documents\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "idf = vectorizer.idf_\n",
    "weights_dict = dict(zip(vectorizer.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Проверка работы TF-IDF модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for test_word in documents[10000:10050]:\n",
    "    amount = 0\n",
    "    similarity = 0\n",
    "    for i in wordpunct_tokenize(test_word):\n",
    "        sim = 0\n",
    "        try:\n",
    "            sim = model.similarity(morph.parse(i)[0].normal_form, keyword)\n",
    "            amount += 1\n",
    "            try:\n",
    "                weight = round(weights_dict[i], 2)\n",
    "                sim *= weight\n",
    "            except:\n",
    "                pass\n",
    "        except KeyError:\n",
    "            pass\n",
    "        similarity += sim\n",
    "    positive = False\n",
    "    sim_am = 0\n",
    "    try:\n",
    "        sim_am = similarity/amount\n",
    "    except ZeroDivisionError:\n",
    "        pass\n",
    "    if sim_am > 0.5:\n",
    "        positive = True\n",
    "    result.append({'text': test_word, 'similarity': sim_am, 'positive': positive})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(df.text):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec(docs, size = 100, window = 5, min_count = 10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
